{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data From xml File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tree = ET.parse('train_data.xml')\n",
    "reviews = tree.getroot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence_array=list()\n",
    "asp_array=list()\n",
    "ref_revarray=list()\n",
    "effect_array=list()\n",
    "revindex=list()\n",
    "i=0\n",
    "j=0\n",
    "prev=0\n",
    "num_sent=0\n",
    "\n",
    "polar = {\n",
    "    \"negative\": 0.0,\n",
    "    \"neutral\":0.5,\n",
    "    \"positive\":1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences are 2508\n",
      "Maximim number of sentences in a review is 44\n",
      "2001\n",
      "2508\n",
      "2508\n",
      "2508\n",
      "2508\n"
     ]
    }
   ],
   "source": [
    "for review in reviews:\n",
    "\n",
    "    if(i==299):\n",
    "        ref_revarray.append(i)\n",
    "        num_sent+=1\n",
    "        effect_array.append(0.0)\n",
    "        asp_array.append('FOOD#QUALITY')\n",
    "        Sentence_array.append(\"Food is very bad\")\n",
    "\n",
    "    sentences = review.find(\"./sentences\")\n",
    "    for sentence in sentences :\n",
    "        num_sent=num_sent+1\n",
    "        text = sentence.find(\"./text\")\n",
    "        opinions = sentence.findall(\"./Opinions/Opinion\")\n",
    "        for opinion in opinions :\n",
    "            if(prev == i):\n",
    "                j=j+1\n",
    "            else:\n",
    "                revindex.append(j)\n",
    "                j=1\n",
    "                prev=i\n",
    "\n",
    "            effect_array.append(polar[opinion.attrib['polarity']])\n",
    "            Sentence_array.append(text.text)\n",
    "            asp_array.append(opinion.attrib['category'])\n",
    "            ref_revarray.append(i)\n",
    "\n",
    "    i=i+1\n",
    "\n",
    "revindex=np.array(revindex)\n",
    "pol_arr=np.array(effect_array)\n",
    "\n",
    "maxi=np.max(revindex)\n",
    "\n",
    "print(\"Total number of sentences are \" + str(len(Sentence_array)))\n",
    "print(\"Maximim number of sentences in a review is \" + str(maxi))\n",
    "\n",
    "\n",
    "\n",
    "print(num_sent)  \n",
    "print(len(Sentence_array))\n",
    "print(len(asp_array))\n",
    "print(len(ref_revarray))\n",
    "print(len(effect_array))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rid</th>\n",
       "      <th>aspect</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>FOOD#STYLE_OPTIONS</td>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>22</td>\n",
       "      <td>AMBIENCE#GENERAL</td>\n",
       "      <td>Leon is an East Village gem: casual but hip, w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>22</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>Leon is an East Village gem: casual but hip, w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>22</td>\n",
       "      <td>AMBIENCE#GENERAL</td>\n",
       "      <td>Leon is an East Village gem: casual but hip, w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>22</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>Leon is an East Village gem: casual but hip, w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>22</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>My wife and I always enjoy the young, not alwa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rid              aspect  \\\n",
       "0      0  RESTAURANT#GENERAL   \n",
       "1      0     SERVICE#GENERAL   \n",
       "2      0     SERVICE#GENERAL   \n",
       "3      0        FOOD#QUALITY   \n",
       "4      0  FOOD#STYLE_OPTIONS   \n",
       "..   ...                 ...   \n",
       "145   22    AMBIENCE#GENERAL   \n",
       "146   22        FOOD#QUALITY   \n",
       "147   22    AMBIENCE#GENERAL   \n",
       "148   22        FOOD#QUALITY   \n",
       "149   22     SERVICE#GENERAL   \n",
       "\n",
       "                                                  text  polarity  \n",
       "0    Judging from previous posts this used to be a ...       0.0  \n",
       "1    We, there were four of us, arrived at noon - t...       0.0  \n",
       "2    They never brought us complimentary noodles, i...       0.0  \n",
       "3    The food was lousy - too sweet or too salty an...       0.0  \n",
       "4    The food was lousy - too sweet or too salty an...       0.0  \n",
       "..                                                 ...       ...  \n",
       "145  Leon is an East Village gem: casual but hip, w...       1.0  \n",
       "146  Leon is an East Village gem: casual but hip, w...       1.0  \n",
       "147  Leon is an East Village gem: casual but hip, w...       1.0  \n",
       "148  Leon is an East Village gem: casual but hip, w...       1.0  \n",
       "149  My wife and I always enjoy the young, not alwa...       1.0  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=list()\n",
    "\n",
    "for i in range(len(Sentence_array)):\n",
    "  data.append([ref_revarray[i], asp_array[i],Sentence_array[i],effect_array[i]])\n",
    "  \n",
    "# Create the pandas DataFrame\n",
    "DATA = pd.DataFrame(data, columns = ['rid','aspect','text','polarity'])\n",
    "\n",
    "DATA[0:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sriramprasad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def rem_stopwords(text):\n",
    "    text1 = [w for w in text.split() if not w.lower() in stop_words]\n",
    "    return \" \".join(text1)\n",
    "\n",
    "def tokenise_data(samp_text):\n",
    "    word_tokens = [word.lower() for word in word_tokenize(samp_text)]\n",
    "    return word_tokens\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", string)\n",
    "\n",
    "def remove_punct(text):\n",
    "  table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "  return text.translate(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/sriramprasad/.local/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow in /home/sriramprasad/.local/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (22.11.23)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: packaging in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorflow) (0.28.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sriramprasad/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sriramprasad/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/sriramprasad/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/sriramprasad/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/sriramprasad/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/sriramprasad/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/sriramprasad/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sriramprasad/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.10.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum words in a sentence are 35\n",
      "maximum sentences in a review are44\n",
      "Total Number Of Reviews 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3703/2397471873.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  corpus=np.array(corpus)\n"
     ]
    }
   ],
   "source": [
    "DATA.text = DATA.text.map(lambda x: remove_URL(x))\n",
    "DATA.text = DATA.text.map(lambda x: remove_html(x))\n",
    "DATA.text = DATA.text.map(lambda x: remove_emoji(x))\n",
    "DATA.text = DATA.text.map(lambda x: remove_punct(x))\n",
    "DATA.text = DATA.text.map(rem_stopwords)\n",
    "\n",
    "hell=list()\n",
    "\n",
    "corpus=list()\n",
    "size=list()\n",
    "\n",
    "\n",
    "hell = DATA.loc[:,\"text\"].tolist()\n",
    "hell1 = DATA.loc[:,\"rid\"].tolist()\n",
    "\n",
    "for i in range(len(Sentence_array)):\n",
    "    corpus.append(tokenise_data(hell[i]))\n",
    "    size.append(len(corpus[i]))\n",
    "\n",
    "\n",
    "corpus=np.array(corpus)\n",
    "hell1 = np.array(hell1)\n",
    "size=np.array(size)\n",
    "\n",
    "maximum= np.max(size)\n",
    "maximum1=np.max(hell1)\n",
    "print(\"maximum words in a sentence are \" + str(maximum))\n",
    "print(\"maximum sentences in a review are\" +str(maxi))\n",
    "print(\"Total Number Of Reviews\",+maximum1+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding And Final Test And Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2058  609 2059 ...    0    0    0]\n",
      " [ 202   34  516 ...    0    0    0]\n",
      " [  17  611   34 ...    0    0    0]\n",
      " ...\n",
      " [ 605  347  292 ...    0    0    0]\n",
      " [ 249   91    7 ...    0    0    0]\n",
      " [ 249   91    7 ...    0    0    0]]\n",
      "[[2058  609 2059 ...    0    0    0]\n",
      " [ 202   34  516 ...    0    0    0]\n",
      " [  17  611   34 ...    0    0    0]\n",
      " ...\n",
      " [ 605  347  292 ...    0    0    0]\n",
      " [ 249   91    7 ...    0    0    0]\n",
      " [ 249   91    7 ...    0    0    0]]\n",
      "<class 'numpy.ndarray'>\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=1000000)\n",
    "tokenizer.fit_on_texts(DATA.text)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(DATA.text)\n",
    "\n",
    "paddedsequences = pad_sequences(\n",
    "sequences, maxlen=35, truncating=\"post\", padding=\"post\"\n",
    ")\n",
    "\n",
    "print(paddedsequences)\n",
    "print(paddedsequences)\n",
    "print(type(paddedsequences[0]))\n",
    "print(len(paddedsequences[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2010, 35)\n",
      "[[2821  647    0 ...    0    0    0]\n",
      " [   2    1    0 ...    0    0    0]\n",
      " [  51  396  519 ...    0    0    0]\n",
      " ...\n",
      " [ 605  347  292 ...    0    0    0]\n",
      " [ 249   91    7 ...    0    0    0]\n",
      " [ 249   91    7 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "final_train_seq=paddedsequences[:2010]\n",
    "final_test_seq=paddedsequences[2010:]\n",
    "\n",
    "final_train_label=pol_arr[:2010]\n",
    "final_test_label=pol_arr[2010:]\n",
    "\n",
    "print(final_train_seq.shape)\n",
    "\n",
    "print(final_test_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fittinf model and caluclating Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 21:33:08.155324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-02 21:33:08.155368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sriram): /proc/driver/nvidia/version does not exist\n",
      "2022-12-02 21:33:08.155854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,SimpleRNN\n",
    "from keras import losses\n",
    "\n",
    "model= Sequential()\n",
    "model.add( Embedding(30000,1,input_length = 35))#embeddings dimension as 32\n",
    "model.add(SimpleRNN(16,input_shape = (2009, 35), return_sequences=False,activation=\"relu\"))\n",
    "model.add(Dense(1, activation='sigmoid'))#Dense layer of 10 units\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 3s 41ms/step - loss: 0.6871 - accuracy: 0.6632 - val_loss: 0.6971 - val_accuracy: 0.4197\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.6707 - accuracy: 0.7204 - val_loss: 0.7032 - val_accuracy: 0.4197\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.6531 - accuracy: 0.7204 - val_loss: 0.7134 - val_accuracy: 0.4197\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.6324 - accuracy: 0.7204 - val_loss: 0.7315 - val_accuracy: 0.4197\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.6082 - accuracy: 0.7204 - val_loss: 0.7694 - val_accuracy: 0.4197\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(final_train_seq,final_train_label,validation_data = (final_test_seq,final_test_label),epochs = 5 ,batch_size= 128,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict(final_test_seq)\n",
    "final_pred=list()\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    if(prediction[i] < 0.5):\n",
    "        final_pred.append(0.0)\n",
    "\n",
    "    else:\n",
    "        final_pred.append(1.0)\n",
    "\n",
    "\n",
    "final_pred=np.array(final_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d417a21d97a2e6b8832d21dc44f5cbe4be6e18325f874a8bb8484565b928cd54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
